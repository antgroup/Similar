import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

device = "cuda"
path = "/mnt/prev_nas/virtual_agent/MBC/RLHF-Reward-Modeling/checkpoints/similar/similar_llava"
model = AutoModelForSequenceClassification.from_pretrained(path, device_map=device, trust_remote_code=True, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True)

# We load a random sample from the validation set of the HelpSteer dataset  我们从HelpSteer数据集的验证集中加载随机样本
prompt = 'What are some synonyms for the word "beautiful"?'
response = "Nicely, Beautifully, Handsome, Stunning, Wonderful, Gorgeous, Pretty, Stunning, Elegant"
# messages = [{"role": "user", "content": prompt},
#            {"role": "assistant", "content": response}]

messages = [{'role': 'system', 'content': [{'type': 'text', 'text': '\nYou are a virtual agent. The Virtual Agent is designed to help a human user complete specified tasks \n(such as app usage, web navigation, web content Q&A, etc.) on various platform applications (such as websites, mobile \ndevices, operation systems, etc.) based on given instructions.\n\nYou will predict the next action based on following content [INSTRUCTION], [OBSERVATION], [REASON_STEPS] and [DIMENSION]:\n1. [INSTRUCTION]: It is your ultimate goal, and all your actions are aimed at completing this task.\n2. [OBSERVATION]: It is an observation of an image, which is the screenshot of the platform (such as computer screen).\n3. [REASON_STEPS]: They are the trajectory of the actions you performed in the past to complete the instruction, from which you can understand how you thought in order to complete the instruction. If it is empty, it means it is currently the first step.\n4. [DIMENSION]: It indicates your tendency to perform actions, and your actions may not necessarily satisfy all dimensions. For example, one of your actions can complete a task but may not be efficient. But when predicting the next action, you need to make predictions based on a given dimension, which may be a single dimension or a combination of all dimensions.\n\n<DIMENSION TYPES>\nPlease evaluate the quality of each step of the Agent operation based on the dimensions provided follow, with scores given in specific dimension. The higher the value, the more satisfied this attribute is.\n1. [INFERENCE POTENTIAL]\n    1.1 Meaning: It indicates the potential of the step to complete the task, which is the probability of a step reaching the completion of the task.\n    1.2 Design motivation: The more correct steps lead to a higher probability of success in the final task, and the more incorrect steps lead to a higher probability of failure in the final task. Different steps have different potential to complete the task. If one step of the agent is to follow the Instructions to complete the task, then this step generally has high potential. We can derive the probability of a step leading to success from the N paths generated by that step, which serves as the potential for that step to complete the task.\n2. [EFFICIENCY]\n    2.1 Meaning: It indicates whether this step is efficient in completing the task. We calculate this metric as the difference between \'the number of steps required to complete the final task after the current step\' and \'the number of steps required to complete the final task after the previous step\', divided by \'the total number of steps required to complete the task\'. This indicates the degree of efficiency improvement in completing tasks after the current step is executed.\n    2.2 Design motivation: A basic assumption is that the fewer steps the Agent operates, the more efficient it is, because the consumption of these paths (time consumption, hardware consumption) can be considered to be the least and the efficiency is the highest. Therefore, if the operation of a step can reduce the number of steps required to complete the task as a whole, then it can be considered that the operation of this step is very efficient. For example, after the previous step, it takes 7 steps to complete the task, but after the current step, it only takes 4 steps to complete the task. The difference of 7-4=3 is the efficiency improvement of the current step in completing the final task. A step that is correct does not necessarily mean it is efficient, but a step that is efficient indicates that it can quickly lead to success, so it is the correct step. Generally speaking, some quick operations (such as using shortcut keys, using history records, etc.) for completing INSTRUCTION can be considered efficient.\n3. [TASK CONTRIBUTION]\n    3.1 Meaning: It indicates the degree to which this step contributes to the completion of the final task. There are good and bad contributions, the correct steps will give a positive contribution, and the wrong steps will give a negative contribution.\n    3.2 Design motivation: Different steps contribute differently to the completion of the final task, with good steps helping to accomplish the task and bad steps hindering it. Good steps should be rewarded positively, while bad steps should be punished negatively. If each step is correct and the total number of steps is 5, then the contribution of each step can be considered as 1/5, meaning that each step completes 1/5 of the final task. If 4 more steps are needed from the current step and the current step is incorrect, then the contribution of the current step is -1/4, indicating that it hinders 1/4 of the final task progress.\n4. [Task Relevance]\n    4.1 Meaning: It indicates is whether the operation of the Agent is related to achieving the INSTRUCTION. \n    4.2 Design motivation: Some operational steps may prevent the task from being completed, but they are related to the task (for example, we need to ask the agent to take notes, and the agent takes notes, which is related to the task, but the recorded note content is incorrect, indicating that this is an incorrect step). Some operational steps may be meaningless, but they can still lead to task completion (such as clicking on a blank screen without generating any response, which is unrelated to the task, but the agent\'s subsequent actions can still result in task success). Therefore, an indicator is needed to identify whether the current step of operation is related to the task.\n5. [Coherence]. \n    5.1 Meaning: It represents the compactness and coherence between the current step and the previous step. \n    5.2 Design motivation: Some operations, although task related, not inefficient, and highly likely to lead to success, lack coherence with the previous step. For example, the task is to "query the Lakers\' game results and record them in the Note". The Agent operations are as follows: a Open the browser; b. Open Note; c. Create new notes; d. Search for Lakers games; e. Query the results of the competition; f. Record the results of the competition in your notes. It can be found that the operations of a and b lack coherence, and it is more in line with human preferences to directly search for competition results after opening the browser instead of simultaneously opening Note.\n6. [Total].\n    Meaning: Integrated decision-making based on the 5 dimensions mentioned earlier\n</DIMENSION TYPES>\n'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': "\n[DIMENSION TYPE]: Inference Potential\n[INSTRUCTION]: In Simple Calendar Pro, create a recurring calendar event titled 'Review session for Budget Planning' starting on 2023-10-15 at 14h. The event recurs weekly, forever, and lasts for 60 minutes each occurrence. The event description should be 'We will understand business objectives. Remember to confirm attendance.'.\n[OBSERVATION]: which is a single image provided.\n[REASON_STEPS]: Step 1 :  open_app Simple Calendar Pro\n\nStep 2 : click New Event\n\nStep 3 : click Event\n\nStep 4 :  input_text Review session for Budget Planning\n\nStep 5 : click 16:00\n\nStep 6 : click 14 hours\n\nStep 7 : click OK\n\nStep 8 : click No repetition\n\nStep 9 : click Weekly\n\nStep 10 :  input_text We will understand business objectives. Remember to confirm attendance.\n\n\n"}, {'type': 'image', 'image': 'https://pic.imgdb.cn/item/6719965ed29ded1a8c609d69.jpg'}]}, {'role': 'assistant', 'content': 'click Save'}]

input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)

with torch.no_grad():
   output = model(input_ids)
   # Multi-objective rewards for the response  响应的多目标奖励
   multi_obj_rewards = output.rewards.cpu().float()
   # The gating layer's output is conditioned on the prompt  门控层的输出取决于提示
   gating_output = output.gating_output.cpu().float()
   # The preference score for the response, aggregated from the multi-objective rewards with the gating layer  响应的偏好得分，由门控层的多目标奖励聚合而成
   preference_score = output.score.cpu().float()

print("multi_obj_rewards = ", multi_obj_rewards)
print("gating_output = ", gating_output)
print("preference_score = ", preference_score)

# We apply a transformation matrix to the multi-objective rewards before multiplying with the gating layer's output. This mainly aims at reducing the verbosity bias of the original reward objectives
# 在与门控层的输出相乘之前，我们将变换矩阵应用于多目标奖励。这主要是为了减少原始奖励目标的冗长偏见
obj_transform = model.reward_transform_matrix.data.cpu().float()
# The final coefficients assigned to each reward objective 分配给每个奖励目标的最终系数
multi_obj_coeffs = gating_output @ obj_transform.T

score = torch.sum(multi_obj_rewards * multi_obj_coeffs, dim=1)
print("score = ", score)
# The preference score is the linear combination of the multi-objective rewards with the multi-objective coefficients, which can be verified by the following assertion
# 偏好得分是多目标奖励与多目标系数的线性组合，可以通过以下断言进行验证
assert torch.isclose(torch.sum(multi_obj_rewards * multi_obj_coeffs, dim=1), preference_score, atol=1e-1)

# Find the top-K reward objectives with coefficients of the highest magnitude  找到系数最高的前K个奖励目标
K = 5
top_obj_dims = torch.argsort(torch.abs(multi_obj_coeffs), dim=1, descending=True,)[:, :K]
top_obj_coeffs = torch.gather(multi_obj_coeffs, dim=1, index=top_obj_dims)

# The attributes of the 5 reward objectives  5个奖励目标的属性
attributes = [
    "IP",
    "E",
    "TC",
    "TR",
    "C",
]


example_index = 0
for i in range(K):
   attribute = attributes[top_obj_dims[example_index, i].item()]
   coeff = top_obj_coeffs[example_index, i].item()
   print(f"{attribute}: {round(coeff, 5)}")
# code-complexity: 0.19922
# helpsteer-verbosity: -0.10864
# ultrafeedback-instruction_following: 0.07861


# The actual rewards of this example from the HelpSteer dataset are [3,3,4,2,2] for the five helpsteer objectives:
# helpfulness, correctness, coherence, complexity, verbosity
# We can linearly transform our predicted rewards to the original reward space to compare with the ground truth
# helpsteer_rewards_pred = multi_obj_rewards[0, :5] * 5 - 0.5
# print(helpsteer_rewards_pred)
# [2.78125   2.859375  3.484375  1.3847656 1.296875 ]