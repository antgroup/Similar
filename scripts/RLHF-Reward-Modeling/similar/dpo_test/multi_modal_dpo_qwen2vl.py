from datasets import features, load_dataset
from six import BytesIO
from transformers import AutoModelForVision2Seq, AutoProcessor
import torch
from trl import DPOConfig, DPOTrainer
from peft import get_peft_model, LoraConfig

import requests
from PIL import Image

messages = [
    {
        'role': 'system',
        'content': [
            {
                'type': 'text',
                'text': '\nYou are a virtual agent. The Virtual Agent is designed to help a human user complete specified tasks \n(such as app usage, web navigation, web content Q&A, etc.) on various platform applications (such as websites, mobile \ndevices, operation systems, etc.) based on given instructions.\n\nYou will predict the next action based on following content [INSTRUCTION], [OBSERVATION], [REASON_STEPS] and [DIMENSION]:\n1. [INSTRUCTION]: It is your ultimate goal, and all your actions are aimed at completing this task.\n2. [OBSERVATION]: It is an observation of an image, which is the screenshot of the platform (such as computer screen).\n3. [REASON_STEPS]: They are the trajectory of the actions you performed in the past to complete the instruction, from which you can understand how you thought in order to complete the instruction. If it is empty, it means it is currently the first step.\n4. [DIMENSION]: It indicates your tendency to perform actions, and your actions may not necessarily satisfy all dimensions. For example, one of your actions can complete a task but may not be efficient. But when predicting the next action, you need to make predictions based on a given dimension, which may be a single dimension or a combination of all dimensions.\n\n<DIMENSION TYPES>\nPlease evaluate the quality of each step of the Agent operation based on the dimensions provided follow, with scores given in specific dimension. The higher the value, the more satisfied this attribute is.\n1. [INFERENCE POTENTIAL]\n    1.1 Meaning: It indicates the potential of the step to complete the task, which is the probability of a step reaching the completion of the task.\n    1.2 Design motivation: The more correct steps lead to a higher probability of success in the final task, and the more incorrect steps lead to a higher probability of failure in the final task. Different steps have different potential to complete the task. If one step of the agent is to follow the Instructions to complete the task, then this step generally has high potential. We can derive the probability of a step leading to success from the N paths generated by that step, which serves as the potential for that step to complete the task.\n2. [EFFICIENCY]\n    2.1 Meaning: It indicates whether this step is efficient in completing the task. We calculate this metric as the difference between \'the number of steps required to complete the final task after the current step\' and \'the number of steps required to complete the final task after the previous step\', divided by \'the total number of steps required to complete the task\'. This indicates the degree of efficiency improvement in completing tasks after the current step is executed.\n    2.2 Design motivation: A basic assumption is that the fewer steps the Agent operates, the more efficient it is, because the consumption of these paths (time consumption, hardware consumption) can be considered to be the least and the efficiency is the highest. Therefore, if the operation of a step can reduce the number of steps required to complete the task as a whole, then it can be considered that the operation of this step is very efficient. For example, after the previous step, it takes 7 steps to complete the task, but after the current step, it only takes 4 steps to complete the task. The difference of 7-4=3 is the efficiency improvement of the current step in completing the final task. A step that is correct does not necessarily mean it is efficient, but a step that is efficient indicates that it can quickly lead to success, so it is the correct step. Generally speaking, some quick operations (such as using shortcut keys, using history records, etc.) for completing INSTRUCTION can be considered efficient.\n3. [TASK CONTRIBUTION]\n    3.1 Meaning: It indicates the degree to which this step contributes to the completion of the final task. There are good and bad contributions, the correct steps will give a positive contribution, and the wrong steps will give a negative contribution.\n    3.2 Design motivation: Different steps contribute differently to the completion of the final task, with good steps helping to accomplish the task and bad steps hindering it. Good steps should be rewarded positively, while bad steps should be punished negatively. If each step is correct and the total number of steps is 5, then the contribution of each step can be considered as 1/5, meaning that each step completes 1/5 of the final task. If 4 more steps are needed from the current step and the current step is incorrect, then the contribution of the current step is -1/4, indicating that it hinders 1/4 of the final task progress.\n4. [Task Relevance]\n    4.1 Meaning: It indicates is whether the operation of the Agent is related to achieving the INSTRUCTION. \n    4.2 Design motivation: Some operational steps may prevent the task from being completed, but they are related to the task (for example, we need to ask the agent to take notes, and the agent takes notes, which is related to the task, but the recorded note content is incorrect, indicating that this is an incorrect step). Some operational steps may be meaningless, but they can still lead to task completion (such as clicking on a blank screen without generating any response, which is unrelated to the task, but the agent\'s subsequent actions can still result in task success). Therefore, an indicator is needed to identify whether the current step of operation is related to the task.\n5. [Coherence]. \n    5.1 Meaning: It represents the compactness and coherence between the current step and the previous step. \n    5.2 Design motivation: Some operations, although task related, not inefficient, and highly likely to lead to success, lack coherence with the previous step. For example, the task is to "query the Lakers\' game results and record them in the Note". The Agent operations are as follows: a Open the browser; b. Open Note; c. Create new notes; d. Search for Lakers games; e. Query the results of the competition; f. Record the results of the competition in your notes. It can be found that the operations of a and b lack coherence, and it is more in line with human preferences to directly search for competition results after opening the browser instead of simultaneously opening Note.\n6. [Total].\n    Meaning: Integrated decision-making based on the 5 dimensions mentioned earlier\n</DIMENSION TYPES>\n'
            }
        ]
    },
    {
        'role': 'user',
        'content': [
            {
                'type': 'text',
                'text': '\n[DIMENSION TYPE]: Efficiency\n[INSTRUCTION]: Add the following expenses into the pro expense:\nExpense: Therapy Sessions\n amount_dollars: $307.01\n category_name: Health Care\n note: I may repeat this\n\n[OBSERVATION]: which is a single image provided.\n[REASON_STEPS]: Step 1 : click Search\n\nStep 2 :  input_text Pro Expense\n\nStep 3 : click Pro Expense\n\nStep 4 : Clicked the "+" button to add a new expense entry, successfully navigated to the "Expense Entry" screen.\n\nStep 5 :  input_text Therapy Sessions\n\nStep 6 :  input_text 307.01\n\n\n'
            },
            {
                'type': 'image',
                'image': 'https://pic.imgdb.cn/item/6719427ed29ded1a8c384b77.jpg'
            }
        ]
    },
    {
        'role': 'assistant', 'content': 'Scrolled right to reveal more categories; "Health Care" is now visible.'
    }
]

def main():
    # Load the model and processor
    model = AutoModelForVision2Seq.from_pretrained("/mnt/prev_nas/virtual_agent/models/Qwen/Qwen2-VL-7B-Instruct", torch_dtype=torch.bfloat16)
    processor = AutoProcessor.from_pretrained("/mnt/prev_nas/virtual_agent/models/Qwen/Qwen2-VL-7B-Instruct", do_image_splitting=False)

    # Load the dataset
    dataset = load_dataset("csv", data_files="/mnt/prev_nas/virtual_agent/MBC/Reward_Model_csv_zeta/Reward_Model_preference_dataset_train_prompt_simple.csv", split="train")

    def format(example):
        # Prepare the input for the chat template
        chosen_message = eval(example["chosen"])
        rejected_message = eval(example["rejected"])
        instruction = chosen_message[0]["content"][0]["text"] + '\n' + chosen_message[1]["content"][0]["text"]
        prompt = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": instruction}]}]
        chosen = [{"role": "assistant", "content": [{"type": "text", "text": chosen_message[2]["content"]}]}]
        rejected = [{"role": "assistant", "content": [{"type": "text", "text": rejected_message[2]["content"]}]}]
        # Apply the chat template
        prompt = processor.apply_chat_template(prompt, tokenize=False)
        chosen = processor.apply_chat_template(chosen, tokenize=False)
        rejected = processor.apply_chat_template(rejected, tokenize=False)
        # Resize the image to ensure it fits within the maximum allowable
        # size of the processor to prevent OOM errors.
        # max_size = processor.image_processor.size["longest_edge"]// 2
        # example["image"].thumbnail((max_size, max_size))
        image_url = chosen_message[1]["content"][1]["image"]
        image = Image.open(BytesIO(requests.get(image_url).content))
        image.thumbnail((256, 256))
        return {"images": [image], "prompt": prompt, "chosen": chosen, "rejected": rejected}

    # Apply the formatting function to the dataset
    dataset = dataset.map(format, remove_columns=dataset.column_names, num_proc=32)

    # Make sure that the images are decoded, it prevents from storing bytes.
    # More info here https://github.com/huggingface/blog/pull/2148#discussion_r1667400478
    f = dataset.features
    f["images"] = features.Sequence(features.Image(decode=True))
    dataset = dataset.cast(f)

    # Train the model
    training_args = DPOConfig(
        output_dir="/mnt/prev_nas/virtual_agent/MBC/RLHF-Reward-Modeling/checkpoints/multi_modal_dpo_qwen2vl",
        bf16=True,
        gradient_checkpointing=False,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=64,
        num_train_epochs=1,
        dataset_num_proc=32, # tokenization will use 32 processes
        dataloader_num_workers=32, # data loading will use 32 workers
        logging_steps=10,
    )

    trainer = DPOTrainer(
        model,
        ref_model=None, # not needed when using peft
        args=training_args,
        train_dataset=dataset,
        tokenizer=processor,
        peft_config=LoraConfig(target_modules=["q_proj", "v_proj"]),
    )

    trainer.train()

if __name__ == "__main__":
    main()